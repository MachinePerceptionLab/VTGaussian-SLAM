<p align="center">

  <h1 align="center">VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians</h1>
  <p align="center">
    <a href="https://pengchongh.github.io/">Pengchong Hu</a>
    ·
    <a href="https://h312h.github.io/">Zhizhong Han</a>

  </p>
  <h2 align="center">ICML 2025</h2>
  <h3 align="center"><a href="https://arxiv.org/pdf/2506.02741">Paper</a> | <a href="https://machineperceptionlab.github.io/VTGaussian-SLAM-Project/">Project Page</a> </h3>
  <div align="center"></div>
</p>

<!-- TABLE OF CONTENTS -->
<details open="open" style='padding: 10px; border-radius:5px 30px 30px 5px; border-width: 1px;'>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#installation">Installation</a>
    </li>
    <li>
      <a href="#dataset">Dataset</a>
    </li>
    <li>
      <a href="#run">Run</a>
    </li>
    <li>
      <a href="#evaluation">Evaluation</a>
    </li>
    <li>
      <a href="#acknowledgement">Acknowledgement</a>
    </li>
    <li>
      <a href="#citation">Citation</a>
    </li>
  </ol>
</details>


## Installation
Please install all dependencies by following the instructions here. You can use [anaconda](https://www.anaconda.com/) and [pip](https://pypi.org/project/pip/) to finish the installation easily. We tested VTGaussian-SLAM on RTX 3090 and RTX 4090 GPUs with Python 3.10, Torch 1.12.1, and CUDA=11.6.

You can build a conda environment called `vtgaussian-slam` following the instructions below.

```bash
git clone https://github.com/MachinePerceptionLab/VTGaussian-SLAM.git
cd VTGaussian-SLAM

conda create -n vtgaussian-slam python=3.10
conda activate vtgaussian-slam
conda install -c "nvidia/label/cuda-11.6.0" cuda-toolkit
conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.6 -c pytorch -c conda-forge
pip install -r requirements.txt
```

## Dataset

DATAROOT is `./data` by default. Please change the input_folder path in the scene-specific config files if datasets are stored somewhere else on your machine.

### Replica

Please download the Replica dataset generated by the authors of iMAP into `./data/Replica` folder. Please cite iMAP if you use the dataset.

```bash
bash bash_scripts/download_replica.sh # Released by authors of NICE-SLAM
```

### TUM-RGBD

```bash
bash bash_scripts/download_tum.sh
```

### ScanNet

Please follow the data downloading procedure on [ScanNet](http://www.scan-net.org/) website, and extract color/depth frames from the `.sens` file using this [code](https://github.com/ScanNet/ScanNet/blob/master/SensReader/python/reader.py).

<details>
  <summary>[Directory structure of ScanNet (click to expand)]</summary>
  
  DATAROOT is `./data` by default. If a sequence (`sceneXXXX_XX`) is stored in other places, please change the `basedir` path in the config file.

```
  DATAROOT
  └── scannet
      └── scene0000_00
          ├── color
          │   ├── 0.jpg
          │   ├── 1.jpg
          │   ├── ...
          │   └── ...
          ├── depth
          │   ├── 0.png
          │   ├── 1.png
          │   ├── ...
          │   └── ...
          ├── intrinsic
          └── pose
              ├── 0.txt
              ├── 1.txt
              ├── ...
              └── ...

```
</details>

### ScanNet++

Please follow the data downloading and image undistortion procedure on the [ScanNet++](https://kaldir.vc.in.tum.de/scannetpp/) website. Additionally, for undistorting the DSLR depth images, we use [a variant of the official ScanNet++ processing code](https://github.com/Nik-V9/scannetpp) from SplaTAM. 

## Run

### Replica
To run VTGaussian-SLAM on the `room0` scene, please use the following command:

```bash
python src/vtgaussian_slam.py configs/replica/room0.py
```

### TUM-RGBD
To run VTGaussian-SLAM on the `freiburg1_desk` scene, please use the following command:

```bash
python src/vtgaussian_slam.py configs/tum/fr1_config.py
```

### ScanNet
To run VTGaussian-SLAM on the `scene0000_00` scene, please use the following command:

```bash
python src/vtgaussian_slam.py configs/scannet/scene00_config.py
```

### ScanNet++
To run VTGaussian-SLAM on the `2e74812d00` scene, please use the following command:

```bash
python src/vtgaussian_slam.py configs/scannetpp/2e_config.py
```




## Evaluation
When running VTGaussian-SLAM using the above commands, we can get the final evaluation results in rendering and tracking. If you want to evaluate it with the saved `params_ls.npy`, you can set the `eval_mode` in the scene-specific config files to True and run the above commands again.

## Acknowledgement

This project was partially supported by an NVIDIA academic award and a Richard Barber research award.

We adapt codes from some awesome repositories, including [3D Gaussian Splatting](https://github.com/graphdeco-inria/gaussian-splatting), [GradSLAM & ConceptFusion](https://github.com/gradslam/gradslam/tree/conceptfusion), [SplaTAM](https://github.com/spla-tam/SplaTAM), and [Gaussian-SLAM](https://github.com/VladimirYugay/Gaussian-SLAM). Thanks for making the code available and for prompt responses to our inquiries regarding the details of their methods.



## Citation
If you find our code or paper useful, please cite
```bibtex
@InProceedings{Hu2025VTGSSLAM,
                title = {VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians},
                author = {Hu, Pengchong and Han, Zhizhong},
                booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
                year = {2025}
}
```
